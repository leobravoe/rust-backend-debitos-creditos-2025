# ==========================================================================================
# ARQUIVO NGINX — VISÃO GERAL PARA INICIANTES
# Este arquivo configura o Nginx para atuar como um “porteiro” extremamente eficiente:
# ele recebe muitas requisições pequenas HTTP/1.1 e repassa para duas instâncias de
# aplicação (app1 e app2). As escolhas abaixo priorizam alto throughput (muitas
# respostas por segundo) mesmo com CPU/RAM limitados, e evitam trabalhos desnecessários
# como escrita de logs durante o benchmark. Cada diretiva é explicada logo acima dela.
# ==========================================================================================

# Define quantos processos “worker” do Nginx serão criados. “auto” pede ao Nginx para
# escolher automaticamente com base no número de CPUs. Em cenários com limitação de
# CPU (ex.: 0.2 vCPU no Docker), isso normalmente resulta em 1 worker — suficiente e
# mais previsível sob restrição. Manter 1 worker evita disputa de CPU entre vários.
worker_processes auto;

# Ajusta o teto de descritores de arquivo (FDs) por worker. Cada conexão aberta (cliente
# ou upstream) consome FDs; como o Nginx atua como proxy, uma requisição típica usa ~2 FDs
# (um para o cliente e outro para o backend). Reservar 20k FDs dá folga para muitos
# clientes simultâneos sem erros “too many open files”, desde que o sistema permita isso.
worker_rlimit_nofile 20000;

# Desliga o log de erro (envia para /dev/null). Em testes de carga, cada escrita em disco
# vira gargalo e ruído na medição. Se você precisar depurar, aponte para um arquivo e
# reative temporariamente. Aqui, como o objetivo é medir desempenho puro, cortamos I/O.
error_log /dev/null;

events {
    # Limita quantas conexões simultâneas cada worker pode manter. Com 8192, um único
    # worker consegue segurar milhares de clientes. Em proxy reverso, isso precisa estar
    # alinhado ao limite de FDs do worker e às capacidades de CPU/RAM do host.
    worker_connections 8192;

    # Informa ao Nginx para usar o mecanismo de I/O mais eficiente do Linux. “epoll”
    # reduz overhead ao acordar o worker somente quando há algo de fato para ler/escrever,
    # melhorando escalabilidade sob alta concorrência.
    use epoll;

    # Permite aceitar várias conexões de uma só vez quando o kernel sinaliza atividade.
    # Isso diminui latência em picos porque reduz o vai-e-volta entre kernel e processo.
    multi_accept on;
}

http {
    # Desativa o log de acesso (cada linha por requisição) para poupar CPU e disco. Em
    # benchmark, logar acessos costuma distorcer resultados e criar “caudas” de latência.
    access_log off;

    # Essas duas opções afinam o comportamento TCP para respostas rápidas:
    # - tcp_nopush agrupa cabeçalhos para reduzir pacotes;
    # - tcp_nodelay envia dados sem esperar encher o buffer (bom para respostas pequenas).
    tcp_nopush on;
    tcp_nodelay on;

    # Mantém a conexão do cliente aberta após a resposta por até 65s (keep-alive). Assim,
    # múltiplas requisições reutilizam a mesma conexão, economizando handshakes e CPU.
    keepalive_timeout 65s;

    # Define quantas requisições um mesmo cliente pode enviar na mesma conexão persistente.
    # Um número alto reduz custos de criação de conexões em cenários de muitas requisições.
    keepalive_requests 10000;
    
    # Quando uma conexão excede timeout, este ajuste força o fechamento imediato, liberando
    # recursos de forma agressiva para atender novos clientes com mais rapidez.
    reset_timedout_connection on;

    # Bloco que define o “grupo” de servidores de backend para onde o Nginx encaminha as
    # requisições. Aqui, temos duas instâncias (app1 e app2) e um pool de conexões persistentes.
    upstream api_backend {
        # Cada linha “server” aponta para uma instância da aplicação atrás do Nginx. O
        # balanceamento padrão é round-robin (alternando entre elas de forma uniforme).
        server app1:3001;
        server app2:3002;

        # Mantém conexões HTTP/1.1 abertas e reutilizáveis entre Nginx e os backends.
        # Isso elimina o custo de abrir/fechar TCP a cada chamada e aumenta o throughput.
        keepalive 510;
    }

    server {
        # Porta pública onde o Nginx escuta. A partir daqui, o mundo externo fala com o
        # Nginx (ex.: http://localhost:9999) e ele distribui para app1/app2 conforme acima.
        listen 9999;

        location / {
            # Liga o buffering de resposta do backend. O Nginx lê a resposta rapidamente
            # do app, libera a conexão com o backend e, então, envia ao cliente do jeito
            # mais eficiente possível. Em cargas altas, isso aumenta a taxa de requisições.
            proxy_buffering on;

            # Garante HTTP/1.1 entre Nginx e backend (necessário para keep-alive no upstream),
            # e limpa o cabeçalho Connection para permitir reutilização automática de conexões.
            proxy_http_version 1.1;
            proxy_set_header Connection "";

            # A linha que finalmente encaminha a requisição recebida para o grupo de
            # backends definido no bloco “upstream” acima.
            proxy_pass http://api_backend;
        }
    }
}